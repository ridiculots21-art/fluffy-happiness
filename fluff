import pandas as pd

# aggregate total demand per key
key_totals = (
    so_pd.groupby("key")["so_nw_ct"]
    .sum()
    .sort_values(ascending=False)
    .reset_index()
)

# cumulative share
key_totals["cum_share"] = key_totals["so_nw_ct"].cumsum() / key_totals["so_nw_ct"].sum()

# select keys up to 70%
pareto_70_keys = key_totals.loc[key_totals["cum_share"] <= 0.70, "key"]

# filter dataset
so_pd_70 = so_pd[so_pd["key"].isin(pareto_70_keys)].copy()





start_test_period = datetime(2025, 7, 1)

rs_base_70, perf_base_70 = run_experiment_over_zones(
    so_pd_70, start_test_period,
    test_period_count=6,
    train_window_months=None,
    oversample_cfg=None,
    save_prefix="LR_baseline_70"
)

rs_1y_70, perf_1y_70 = run_experiment_over_zones(
    so_pd_70, start_test_period,
    test_period_count=6,
    train_window_months=12,
    oversample_cfg=None,
    save_prefix="LR_1yr_70"
)

oversample_cfg_weight = {"strategy":"weight", "weight_factor":5.0}

rs_os_70, perf_os_70 = run_experiment_over_zones(
    so_pd_70, start_test_period,
    test_period_count=6,
    train_window_months=None,
    oversample_cfg=oversample_cfg_weight,
    save_prefix="LR_os_70"
)

rs_1y_os_70, perf_1y_os_70 = run_experiment_over_zones(
    so_pd_70, start_test_period,
    test_period_count=6,
    train_window_months=12,
    oversample_cfg=oversample_cfg_weight,
    save_prefix="LR_1yr_os_70"
)



import polars as pl

perf_base_70 = perf_base_70.with_columns(model=pl.lit("Baseline"))
perf_1y_70 = perf_1y_70.with_columns(model=pl.lit("1-Year"))
perf_os_70 = perf_os_70.with_columns(model=pl.lit("OS"))
perf_1y_os_70 = perf_1y_os_70.with_columns(model=pl.lit("1-Year + OS"))

comp_70 = pl.concat([perf_base_70, perf_1y_70, perf_os_70, perf_1y_os_70])

summary_70 = comp_70.group_by("model").agg(
    pl.mean("mape_avg").alias("mean_mape"),
    pl.median("mape_avg").alias("median_mape")
).sort("median_mape")

display(summary_70)



import matplotlib.pyplot as plt

plot_df_70 = comp_70.select(["model", "mape_avg"]).to_pandas()

plt.figure(figsize=(9,6))

plot_df_70.boxplot(
    column="mape_avg",
    by="model",
    grid=False
)

plt.suptitle("")
plt.title("Pareto 70 â€“ Per-Key MAPE Distribution")






summary_80 = summary_80.with_columns(pareto=pl.lit("80"))
summary_70 = summary_70.with_columns(pareto=pl.lit("70"))

compare_pareto = pl.concat([summary_80, summary_70]).sort(["model", "pareto"])
display(compare_pareto)



comp_80 = comp_80.with_columns(pareto=pl.lit("80"))
comp_70 = comp_70.with_columns(pareto=pl.lit("70"))

combined = pl.concat([comp_80, comp_70])
plot_df = combined.select(["model", "pareto", "mape_avg"]).to_pandas()




plt.figure(figsize=(10,6))

for (model, pareto), group in plot_df.groupby(["model", "pareto"]):
    plt.hist(group["mape_avg"], bins=30, alpha=0.4, label=f"{model}-{pareto}")

plt.legend()
plt.xlabel("MAPE")
plt.ylabel("Count")
plt.title("Pareto 70 vs 80 Comparison")
plt.show()

plt.ylabel("MAPE")
plt.xticks(rotation=45)
plt.show()






from multiprocessing import Pool

def run_experiment_over_zones(...):

    if not isinstance(so_fcst_df, pd.DataFrame):
        so_fcst_df = so_fcst_df.to_pandas()

    # build lags ONCE globally
    so_fcst_df = ensure_lags(so_fcst_df)

    test_period_list = [
        start_test_period + relativedelta(months=i)
        for i in range(test_period_count)
    ]

    jobs = []
    for zone in np.sort(so_fcst_df["zone"].unique()):
        zone_df = so_fcst_df.loc[so_fcst_df["zone"] == zone].copy()
        zone_df = zone_df.drop(columns=["zone"], errors="ignore")

        for test_period in test_period_list:
            jobs.append(
                (zone_df, test_period, train_window_months,
                 oversample_cfg, min_train_size)
            )

    print(f"Running {len(jobs)} jobs using 18 processes")

    with Pool(18) as pool:
        parts = pool.map(run_zone_forecasting_lr_experiment, jobs)

    final = pd.concat(parts, ignore_index=True)
---------------------------------------------------------------------------------------------------------------
# Polars: overall zero vs nonzero, per-zone, per-key
import polars as pl
import matplotlib.pyplot as plt

# use existing so_fcst_df (polars)
df = so_fcst_df

# overall counts
overall = df.with_columns(
    is_zero = (pl.col("so_nw_ct") == 0).cast(pl.Int32)
).select([
    pl.sum("is_zero").alias("n_zero"),
    (pl.count() - pl.sum("is_zero")).alias("n_nonzero"),
    pl.count().alias("n_total")
]).with_columns(
    pl.col("n_zero")/pl.col("n_total").alias("zero_ratio")
)
print("Overall (zeros vs non-zero):")
print(overall.to_pandas())

# per-zone zero ratio
zone_zero = df.with_columns(
    is_zero = (pl.col("so_nw_ct") == 0).cast(pl.Int32)
).group_by("zone").agg(
    pl.sum("is_zero").alias("n_zero"),
    (pl.count() - pl.sum("is_zero")).alias("n_nonzero"),
    pl.count().alias("n_total")
).with_columns(
    (pl.col("n_zero")/pl.col("n_total")).alias("zero_ratio"),
    (pl.col("n_nonzero")/pl.col("n_total")).alias("nonzero_ratio")
).sort("zero_ratio", descending=True)

zone_zero.write_csv("zero_ratio_by_zone.csv")
print("Saved zero_ratio_by_zone.csv")

# per-key zero ratio (this is the most important)
key_zero = df.with_columns(
    is_zero = (pl.col("so_nw_ct") == 0).cast(pl.Int32)
).group_by("key").agg(
    pl.sum("is_zero").alias("n_zero"),
    (pl.count() - pl.sum("is_zero")).alias("n_nonzero"),
    pl.count().alias("n_total")
).with_columns(
    (pl.col("n_zero")/pl.col("n_total")).alias("zero_ratio"),
    pl.col("n_nonzero").cast(pl.Int64).alias("nonzero_count")
).sort("zero_ratio", descending=True)

key_zero.write_csv("zero_ratio_by_key.csv")
print("Saved zero_ratio_by_key.csv")

# Quick stats for key_zero
print(key_zero.describe().to_pandas())

# Visual: histogram of key zero_ratio (to see how many keys are dominated by zeros)
kpd = key_zero.select(["key","zero_ratio"]).to_pandas()
plt.figure(figsize=(8,4))
plt.hist(kpd["zero_ratio"], bins=50)
plt.xlabel("Zero ratio per key")
plt.ylabel("Number of keys")
plt.title("Distribution of zero_ratio across keys")
plt.show()

# Also: count keys with >80% zeros, >90% zeros, all zeros
cnts = {
    "keys_all_zero": (kpd["zero_ratio"] == 1.0).sum(),
    "keys_gt90": (kpd["zero_ratio"] > 0.90).sum(),
    "keys_gt80": (kpd["zero_ratio"] > 0.80).sum(),
    "total_keys": len(kpd)
}
print("Key-level zero stats:", cnts)


----------------

import pandas as pd
import matplotlib.pyplot as plt
so = so_pd   # your pandas df

overall = pd.Series({
    "n_zero": (so["so_nw_ct"] == 0).sum(),
    "n_nonzero": (so["so_nw_ct"] != 0).sum(),
    "n_total": len(so)
})
overall["zero_ratio"] = overall["n_zero"] / overall["n_total"]
print(overall)

zone_zero = so.assign(is_zero=so["so_nw_ct"] == 0).groupby("zone").agg(
    n_zero=("is_zero","sum"),
    n_total=("is_zero","size")
).assign(
    n_nonzero=lambda d: d["n_total"] - d["n_zero"],
    zero_ratio=lambda d: d["n_zero"]/d["n_total"]
).sort_values("zero_ratio", ascending=False)
zone_zero.to_csv("zero_ratio_by_zone.csv")
print("Saved zero_ratio_by_zone.csv")

key_zero = so.assign(is_zero=so["so_nw_ct"] == 0).groupby("key").agg(
    n_zero=("is_zero","sum"),
    n_total=("is_zero","size")
).assign(
    n_nonzero=lambda d: d["n_total"]-d["n_zero"],
    zero_ratio=lambda d: d["n_zero"]/d["n_total"]
).sort_values("zero_ratio", ascending=False)

key_zero.to_csv("zero_ratio_by_key.csv")
print("Saved zero_ratio_by_key.csv")

# plot distribution of zero_ratio
plt.figure(figsize=(8,4))
plt.hist(key_zero["zero_ratio"], bins=50)
plt.xlabel("Zero ratio per key")
plt.ylabel("Number of keys")
plt.title("Distribution of zero_ratio across keys")
plt.show()

print("keys all-zero   :", (key_zero["zero_ratio"] == 1.0).sum())
print("keys > 90% zero :", (key_zero["zero_ratio"] > 0.90).sum())
print("keys > 80% zero :", (key_zero["zero_ratio"] > 0.80).sum())






# top 20 keys with highest zero ratio but with at least 1 non-zero
top20 = key_zero.filter(pl.col("n_nonzero")>0).sort("zero_ratio", descending=True).head(20).to_pandas()
print(top20)

