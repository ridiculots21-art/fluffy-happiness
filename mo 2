leb_2025_df = forecast_final.filter(
    pl.col("periods") == "2025 03"
).select([
    "key",
    "periods",
    "label",
    "ma3",
    "final_forecast",
    "so_nw_ct"
]).sort("key")

print("Rows:", leb_2025_df.height)
leb_2025_df.head(20)




# extract Lebaran-adjusted forecast (March 2025)
leb_forecast = forecast_final.filter(
    pl.col("periods") == "2025 03"
).select([
    "key",
    pl.col("final_forecast").alias("leb_forecast")
])

# join to baseline MA forecast table
ma_adjusted_df = rs_with_error_ma.join(
    leb_forecast,
    on="key",
    how="left"
)

# replace MA forecast only for March 2025
ma_adjusted_df = ma_adjusted_df.with_columns(
    forecast_final = pl.when(pl.col("periods") == "2025 03")
        .then(pl.col("leb_forecast"))
        .otherwise(pl.col("ma3"))
)

ma_adjusted_df.select([
    "key",
    "periods",
    "ma3",
    "leb_forecast",
    "forecast_final"
]).filter(pl.col("periods").is_in([
    "2025 01","2025 02","2025 03","2025 04","2025 05","2025 06"
])).head(20)


















ma_adjusted_perf = ma_adjusted_df.with_columns(
    re = pl.col("forecast_final") - pl.col("so_nw_ct"),
    ae = (pl.col("forecast_final") - pl.col("so_nw_ct")).abs()
).with_columns(
    mape = pl.when(pl.col("so_nw_ct") > 0)
        .then(pl.col("ae") / pl.col("so_nw_ct"))
        .otherwise(1)
).with_columns(
    mape = pl.when(pl.col("mape") > 1)
        .then(1)
        .otherwise(pl.col("mape"))
).filter(
    (pl.col("periods") >= "2025 01") &
    (pl.col("periods") <= "2025 06")
)

ma_adjusted_perf = ma_adjusted_perf.pivot(
    on="periods",
    index="key",
    values="mape",
    aggregate_function="sum",
    sort_columns=True
)

ma_adjusted_perf = ma_adjusted_perf.with_columns(
    mape_avg = ma_adjusted_perf.drop("key").mean_horizontal(),
    pareto80_flag = pl.when(pl.col("key").is_in(pareto_df["key"]))
        .then(1)
        .otherwise(0)
)







baseline_ma_perf = perform_df.select([
    "key",
    "mape_avg",
    "pareto80_flag"
]).with_columns(
    pl.lit("Baseline_MA").alias("model")
)






adjusted_ma_perf = ma_adjusted_perf.select([
    "key",
    "mape_avg",
    "pareto80_flag"
]).with_columns(
    pl.lit("Adjusted_MA").alias("model")
)






ma_compare = pl.concat([
    baseline_ma_perf,
    adjusted_ma_perf
])






pareto_compare_ma = ma_compare.group_by(
    ["model", "pareto80_flag"]
).agg(
    pl.col("mape_avg").mean().alias("avg_mape")
).sort(["model","pareto80_flag"])

pareto_compare_ma







plot_df = pareto_compare_ma.to_pandas()

plt.figure(figsize=(10,5))

sns.barplot(
    data=plot_df,
    x="model",
    y="avg_mape",
    hue="pareto80_flag"
)

plt.title("Baseline MA vs Adjusted MA (Pareto vs Non-Pareto)")
plt.ylabel("Average MAPE")
plt.xlabel("Model")
plt.legend(title="Pareto80")

plt.show()




# Cell: create adjusted_forecast (March replaced), compute per-key MAPEs, compare Pareto vs Non-Pareto
import polars as pl
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# guard: ensure required frames exist
for name in ["forecast_final", "pareto_df"]:
    if name not in globals():
        raise RuntimeError(f"`{name}` not found in memory. Run prior cells that produce it.")

# build adjusted forecast column (safe: don't overwrite existing final_forecast)
forecast_adj = forecast_final.with_columns(
    pl.when(pl.col("periods") == "2025 03")
      .then(pl.col("final_forecast"))
      .otherwise(pl.col("ma3"))
    .alias("adjusted_forecast")
)

# keep only the 2025 Jan-Jun horizon for evaluation (same window you used)
eval_window = ["2025 01","2025 02","2025 03","2025 04","2025 05","2025 06"]
eval_df = (
    forecast_adj
    .filter(pl.col("periods").is_in(eval_window) & pl.col("ma3").is_not_null())
    .with_columns([
        (pl.col("ma3") - pl.col("so_nw_ct")).abs().alias("ae_ma"),
        (pl.col("adjusted_forecast") - pl.col("so_nw_ct")).abs().alias("ae_adj")
    ])
    .with_columns([
        pl.when(pl.col("so_nw_ct") > 0).then(pl.col("ae_ma") / pl.col("so_nw_ct")).otherwise(None).alias("mape_ma"),
        pl.when(pl.col("so_nw_ct") > 0).then(pl.col("ae_adj") / pl.col("so_nw_ct")).otherwise(None).alias("mape_adj")
    ])
)

# average per key
eval_key_df = (
    eval_df
    .group_by("key")
    .agg([
        pl.col("mape_ma").mean().alias("mape_ma_avg"),
        pl.col("mape_adj").mean().alias("mape_adj_avg")
    ])
)

# attach pareto flag
eval_key_df = eval_key_df.with_columns(
    pl.when(pl.col("key").is_in(pareto_df["key"]))
      .then(1)
      .otherwise(0)
    .alias("pareto80_flag")
)

# show some diagnostics
print("Per-key sample (head):")
display(eval_key_df.head(10).to_pandas())

# group by pareto flag and compute average of the per-key averages
grouped = eval_key_df.group_by("pareto80_flag").agg([
    pl.col("mape_ma_avg").mean().alias("ma_avg"),
    pl.col("mape_adj_avg").mean().alias("adj_avg"),
    pl.count().alias("n_keys")
]).sort("pareto80_flag")

print("\nAggregate comparison (Pareto flag):")
display(grouped.to_pandas())

# tidy df for plotting (4 bars)
plot_df = grouped.with_columns([
    pl.col("pareto80_flag").cast(pl.Utf8)
]).melt(id_vars="pareto80_flag", value_vars=["ma_avg","adj_avg"], variable_name="metric", value_name="avg_mape").to_pandas()

plt.figure(figsize=(7,5))
sns.barplot(data=plot_df, x="metric", y="avg_mape", hue="pareto80_flag")
plt.title("Baseline MA vs Adjusted MA â€” average per-key MAPE\n(Grouped: Non-Pareto=0, Pareto=1)")
plt.ylabel("Average MAPE (per-key mean)")
plt.xlabel("")
plt.legend(title="pareto80_flag")
plt.tight_layout()
plt.show()







# Aggregate for plotting
plot_df = (
    eval_key_df
    .group_by("pareto80_flag")
    .agg([
        pl.col("mape_ma_avg").mean().alias("MA3"),
        pl.col("mape_adj_avg").mean().alias("MA3_Adjusted")
    ])
    .sort("pareto80_flag")
    .to_pandas()
)

# Plot
ax = plot_df.set_index("pareto80_flag").plot(
    kind="bar",
    figsize=(8,5)
)

ax.set_title("MA3 vs March Adjusted MAPE\nPareto vs Non-Pareto")
ax.set_ylabel("Average MAPE")
ax.set_xticks([0,1])
ax.set_xticklabels(["Non Pareto", "Pareto"], rotation=0)
ax.legend(["MA3 Baseline", "MA3 Adjusted"])

plt.tight_layout()
plt.show()














# Cell 1: load ARIMA CSV, normalize periods, join pred into forecast_final as `arima_pred`
from pathlib import Path
import polars as pl

ARIMA_FILE = "arime_forecast_2025Q1.csv"   # use exactly the file name you said

# guard: forecast_final must exist
if "forecast_final" not in globals():
    raise RuntimeError("`forecast_final` not found in memory. Run prior cells that create `forecast_final` first.")

# check file presence
p = Path(ARIMA_FILE)
if not p.exists():
    raise RuntimeError(f"ARIMA file not found at path: {ARIMA_FILE}")

# read file
arima_df = pl.read_csv(p)

# normalize column names to expected ones (lowercase)
arima_df = arima_df.rename({c: c.lower() for c in arima_df.columns})

required_cols = {"key", "periods", "so_nw_ct", "pred"}
missing = required_cols - set(arima_df.columns)
if missing:
    raise RuntimeError(f"ARIMA file is missing expected columns: {missing}. Found: {arima_df.columns}")

# normalize periods to "YYYY MM" if necessary (common variants handled)
# Examples handled: "2025-01", "2025_01", "202501", "2025 01"
def normalize_periods(pol_df, col="periods"):
    s = pol_df[col].cast(pl.Utf8)
    # 1) replace '-' or '_' with space
    s = s.str.replace("-", " ").str.replace("_", " ")
    # 2) if format is "YYYYMM" (6 digits) -> insert space
    s = pl.when(s.str.lengths() == 6).then(s.str.slice(0,4) + " " + s.str.slice(4,6)).otherwise(s)
    return s

arima_df = arima_df.with_columns(
    normalize_periods(arima_df, "periods").alias("periods")
)

# cast pred to float
arima_df = arima_df.with_columns(pl.col("pred").cast(pl.Float64))

# diagnostic: unique periods in arima vs forecast_final
arima_periods = arima_df.select(pl.col("periods").unique().alias("arima_periods"))
ff_periods = forecast_final.select(pl.col("periods").unique().alias("ff_periods"))

print("ARIMA rows read:", arima_df.height)
print("Unique ARIMA periods (sample):")
display(arima_periods.head(20).to_pandas())

print("Sample forecast_final periods (sample):")
display(ff_periods.head(20).to_pandas())

# perform left join on key + periods to attach pred
forecast_with_arima = forecast_final.join(
    arima_df.select(["key", "periods", "pred"]),
    on=["key", "periods"],
    how="left"
).with_columns(
    pl.col("pred").alias("arima_pred")  # keep original name `pred` and also alias to arima_pred
)

# diagnostics: how many rows got a join hit
total_rows = forecast_with_arima.height
with_pred = forecast_with_arima.filter(pl.col("arima_pred").is_not_null()).height
without_pred = total_rows - with_pred

print(f"Joined forecast_final rows: {total_rows}")
print(f"Rows with arima_pred (join hit): {with_pred}")
print(f"Rows without arima_pred (no match in ARIMA file): {without_pred}")

print("\nSample rows where arima_pred is present:")
display(forecast_with_arima.filter(pl.col("arima_pred").is_not_null()).select(
    ["key","periods","ma3","final_forecast","arima_pred","so_nw_ct","label"]
).head(20).to_pandas())

# keep the new frame in memory for next cells
globals()["forecast_with_arima"] = forecast_with_arima
