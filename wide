# cell 3
comparison = (
  df
  .with_columns(
      pl.col("year").map_dict(LEBARAN_MONTH).alias("lebaran_month")
  )
  .filter(
      (pl.col("month") == pl.col("lebaran_month")) |
      (pl.col("month") == pl.col("lebaran_month") - 1)
  )
  .group_by(["key", "year", "month"])
  .agg(pl.col("so_nw_ct").sum().alias("sales"))
)

month_cols = sorted([c for c in pivot.columns if isinstance(c, (int, float))])

pivot = pivot.rename(columns={
    month_cols[0]: "before_sales",
    month_cols[1]: "lebaran_sales"
})


# cell 1
for year in [2023, 2024, 2025]:
  lebaran_date = pd.Timestamp(f"{year}-{LEBARAN_MONTH[year]:02d}-01")

# cell 6
pivot["3M - Lebaran"] = [
    pivot.loc[i, "Jan"] - pivot.loc[i, 
        "Apr" if LEBARAN_MONTH[pivot.loc[i, "year"]] == 4 else "Mar"
    ]
    for i in pivot.index
]

pivot["1M - Lebaran"] = [
    pivot.loc[i, "Mar"] - pivot.loc[i, 
        "Apr" if LEBARAN_MONTH[pivot.loc[i, "year"]] == 4 else "Mar"
    ]
    for i in pivot.index
]

# cell 7
pivot["3M - Lebaran"] = [
    (pivot.loc[i, "Jan"] - pivot.loc[i,
        "Apr" if LEBARAN_MONTH[pivot.loc[i, "year"]] == 4 else "Mar"
    ]) /
    (pivot.loc[i,
        "Apr" if LEBARAN_MONTH[pivot.loc[i, "year"]] == 4 else "Mar"
    ] if pivot.loc[i,
        "Apr" if LEBARAN_MONTH[pivot.loc[i, "year"]] == 4 else "Mar"
    ] != 0 else None)
    for i in pivot.index
]


# CONFIG
DEFAULT_LEBARAN_DATES = [
    "2023-04-22",
    "2024-04-10",
    "2025-03-31",
]

CONFIG = {
    "period_col": "periods",     # 'YYYY MM'
    "value_col": "so_nw_ct",
    "group_key": "key",
    "zone_extract_idx": 0,       # index when splitting key by '_' to get zone
    "pareto_keys": None,         # set to list/Series if restricting to pareto
    "filter_soldto": None,
    "filter_shipto": None,
}

------------------------------------------------------------------------

# Cell 2 — core helpers: lebaran -> periods, and compute per-key diffs
def to_period_str(dt: datetime) -> str:
    return dt.strftime("%Y %m")

def lebaran_to_periods(lebaran_dates):
    """Return list of dicts with lebaran_period, m1, m2, m3 for each date string YYYY-MM-DD"""
    periods = []
    for d in lebaran_dates:
        ld = datetime.strptime(d, "%Y-%m-%d")
        periods.append({
            "lebaran_date": ld,
            "lebaran_period": to_period_str(ld),
            "m1": to_period_str(ld - relativedelta(months=1)),
            "m2": to_period_str(ld - relativedelta(months=2)),
            "m3": to_period_str(ld - relativedelta(months=3)),
        })
    return periods

def compute_lebaran_diffs_for_one(so_df: pl.DataFrame, lebaran_info: dict, cfg: dict = CONFIG) -> pl.DataFrame:
    """
    For a single lebaran event, build wide table per key with columns:
      key, zone, m3, m2, m1, lebaran, diff_m1_lebaran, diff_m3_lebaran, diff_m2_m1, diff_m3_m2
    Missing months are filled with 0 (configurable approach).
    """
    period_col = cfg["period_col"]
    val_col = cfg["value_col"]
    key_col = cfg["group_key"]

    required = [lebaran_info["m3"], lebaran_info["m2"], lebaran_info["m1"], lebaran_info["lebaran_period"]]

    tmp = (
        so_df
        .filter(pl.col(period_col).is_in(required))
        .select([pl.col(key_col), pl.col(period_col), pl.col(val_col)])
        .group_by([key_col, period_col])
        .agg(pl.col(val_col).sum().alias(val_col))
    )

    pivoted = tmp.pivot(
        on=period_col,
        index=key_col,
        values=val_col,
        aggregate_function="sum",
        sort_columns=True
    ).fill_null(0)

    # Ensure columns exist even if absent
    for p in required:
        if p not in pivoted.columns:
            pivoted = pivoted.with_columns(pl.lit(0).alias(p))

    pivoted = pivoted.with_columns(
        pl.col(required[0]).alias("m3"),
        pl.col(required[1]).alias("m2"),
        pl.col(required[2]).alias("m1"),
        pl.col(required[3]).alias("lebaran"),
    )

    # extract zone from key (flexible index)
    pivoted = pivoted.with_columns(
        zone = pl.col(key_col).str.split("_").list.get(cfg["zone_extract_idx"])
    )

    pivoted = pivoted.with_columns(
        (pl.col("m1") - pl.col("lebaran")).alias("diff_m1_lebaran"),
        (pl.col("m3") - pl.col("lebaran")).alias("diff_m3_lebaran"),
        (pl.col("m2") - pl.col("m1")).alias("diff_m2_m1"),
        (pl.col("m3") - pl.col("m2")).alias("diff_m3_m2"),
    )

    out = pivoted.select([
        key_col, "zone", "m3", "m2", "m1", "lebaran",
        "diff_m1_lebaran","diff_m3_lebaran","diff_m2_m1","diff_m3_m2"
    ])

    return out

def compute_lebaran_diffs(so_df: pl.DataFrame, lebaran_dates, cfg: dict = CONFIG):
    """
    Runs compute_lebaran_diffs_for_one for each lebaran date.
    Returns:
      - per_event_concat (Polars DF) : per-key per-event diffs (adds column lebaran_period)
      - national_summary_unweighted (Pandas DF) : simple means (unweighted)
      - zone_summary_unweighted (Pandas DF) : simple means per zone (unweighted)
    """
    infos = lebaran_to_periods(lebaran_dates)
    per_event_dfs = []
    national_rows = []
    zone_rows = []

    df = so_df
    if cfg.get("pareto_keys") is not None:
        df = df.filter(pl.col(cfg["group_key"]).is_in(cfg["pareto_keys"]))
    if cfg.get("filter_soldto") is not None:
        df = df.filter(pl.col("key").str.contains(str(cfg["filter_soldto"])))
    if cfg.get("filter_shipto") is not None:
        df = df.filter(pl.col("key").str.contains(str(cfg["filter_shipto"])))

    for info in infos:
        diffs = compute_lebaran_diffs_for_one(df, info, cfg)
        diffs = diffs.with_columns(pl.lit(info["lebaran_period"]).alias("lebaran_period"))
        per_event_dfs.append(diffs)

        nat = diffs.select([
            pl.mean("diff_m1_lebaran").alias("avg_diff_m1_lebaran"),
            pl.mean("diff_m3_lebaran").alias("avg_diff_m3_lebaran"),
            pl.mean("diff_m2_m1").alias("avg_diff_m2_m1"),
            pl.mean("diff_m3_m2").alias("avg_diff_m3_m2")
        ]).with_columns(pl.lit(info["lebaran_period"]).alias("lebaran_period"))
        national_rows.append(nat.to_pandas())

        zone_avg = diffs.group_by("zone").agg([
            pl.mean("diff_m1_lebaran").alias("avg_diff_m1_lebaran"),
            pl.mean("diff_m3_lebaran").alias("avg_diff_m3_lebaran"),
            pl.mean("diff_m2_m1").alias("avg_diff_m2_m1"),
            pl.mean("diff_m3_m2").alias("avg_diff_m3_m2"),
        ]).with_columns(pl.lit(info["lebaran_period"]).alias("lebaran_period"))
        zone_rows.append(zone_avg.to_pandas())

    per_event_concat = pl.concat(per_event_dfs)
    national_summary = pd.concat(national_rows, ignore_index=True)
    zone_summary = pd.concat(zone_rows, ignore_index=True)

    return per_event_concat, national_summary, zone_summary

------------------------------------------------------------------------

# Cell 3 — weighted summaries + example usage

def build_weight_map(so_df: pl.DataFrame, year_str="2025", period_col="periods", key_col="key", val_col="so_nw_ct"):
    """
    Build weight mapping per key using total sales in the year_str (e.g., '2025').
    Returns a Polars DataFrame with columns ['key','sales_{year}'].
    """
    sel = so_df.filter(pl.col(period_col).str.contains(year_str))
    weight_map = sel.group_by(key_col).agg(pl.col(val_col).sum().alias(f"sales_{year_str}"))
    return weight_map

def weighted_summaries(per_event_df: pl.DataFrame, weight_map: pl.DataFrame, weight_col_name: str, 
                       lebaran_period_col="lebaran_period"):
    """
    per_event_df: Polars DF from compute_lebaran_diffs (must include key, zone, diffs, lebaran_period)
    weight_map: Polars DF with ['key', weight_col_name]
    Returns:
      - national_weighted_df (Pandas): weighted averages per lebaran_period
      - zone_weighted_df (Pandas): weighted averages per lebaran_period per zone
    """
    merged = per_event_df.join(weight_map, on="key", how="left").fill_null(0)
    wcol = weight_col_name

    # national numerators/denominators per lebaran_period
    nat = (
        merged
        .group_by(lebaran_period_col)
        .agg([
            (pl.col("diff_m1_lebaran") * pl.col(wcol)).sum().alias("num_m1"),
            (pl.col("diff_m3_lebaran") * pl.col(wcol)).sum().alias("num_m3"),
            (pl.col("diff_m2_m1") * pl.col(wcol)).sum().alias("num_m2_m1"),
            (pl.col("diff_m3_m2") * pl.col(wcol)).sum().alias("num_m3_m2"),
            pl.col(wcol).sum().alias("den")
        ])
        .with_columns([
            pl.when(pl.col("den") == 0).then(None).otherwise(pl.col("num_m1") / pl.col("den")).alias("w_avg_diff_m1_lebaran"),
            pl.when(pl.col("den") == 0).then(None).otherwise(pl.col("num_m3") / pl.col("den")).alias("w_avg_diff_m3_lebaran"),
            pl.when(pl.col("den") == 0).then(None).otherwise(pl.col("num_m2_m1") / pl.col("den")).alias("w_avg_diff_m2_m1"),
            pl.when(pl.col("den") == 0).then(None).otherwise(pl.col("num_m3_m2") / pl.col("den")).alias("w_avg_diff_m3_m2"),
        ])
    )

    # zone-level weighted averages
    zone = (
        merged
        .group_by(["lebaran_period", "zone"])
        .agg([
            (pl.col("diff_m1_lebaran") * pl.col(wcol)).sum().alias("num_m1"),
            (pl.col("diff_m3_lebaran") * pl.col(wcol)).sum().alias("num_m3"),
            (pl.col("diff_m2_m1") * pl.col(wcol)).sum().alias("num_m2_m1"),
            (pl.col("diff_m3_m2") * pl.col(wcol)).sum().alias("num_m3_m2"),
            pl.col(wcol).sum().alias("den")
        ])
        .with_columns([
            pl.when(pl.col("den") == 0).then(None).otherwise(pl.col("num_m1") / pl.col("den")).alias("w_avg_diff_m1_lebaran"),
            pl.when(pl.col("den") == 0).then(None).otherwise(pl.col("num_m3") / pl.col("den")).alias("w_avg_diff_m3_lebaran"),
            pl.when(pl.col("den") == 0).then(None).otherwise(pl.col("num_m2_m1") / pl.col("den")).alias("w_avg_diff_m2_m1"),
            pl.when(pl.col("den") == 0).then(None).otherwise(pl.col("num_m3_m2") / pl.col("den")).alias("w_avg_diff_m3_m2"),
        ])
    )

    return nat.to_pandas(), zone.to_pandas()

# ---------- Example usage (run after previous cells) ----------
# 1) compute per-event diffs (unweighted)
# per_event, national_unweighted, zone_unweighted = compute_lebaran_diffs(so_fcst_df, DEFAULT_LEBARAN_DATES, CONFIG)

# 2) build weight map (2025 sales per key)
# weight_map = build_weight_map(so_fcst_df, year_str="2025", period_col=CONFIG["period_col"], key_col=CONFIG["group_key"], val_col=CONFIG["value_col"])

# 3) call weighted summaries
# national_weighted, zone_weighted = weighted_summaries(per_event, weight_map, weight_col_name="sales_2025")

# 4) save or inspect
# national_weighted.to_csv("national_weighted_lebaran_summary.csv", index=False)
# zone_weighted.to_csv("zone_weighted_lebaran_summary.csv", index=False)

# 5) quick plot example (national weighted)
# display(national_weighted)
# melted = pd.melt(national_weighted, id_vars=["lebaran_period"], value_vars=[
#     "w_avg_diff_m1_lebaran","w_avg_diff_m3_lebaran","w_avg_diff_m2_m1","w_avg_diff_m3_m2"
# ], var_name="metric", value_name="value")
# plt.figure(figsize=(10,4))
# sns.barplot(data=melted, x="lebaran_period", y="value", hue="metric")
# plt.axhline(0, color="k", linewidth=0.6)
# plt.title("Weighted national diffs (weights = 2025 sales per key)")
# plt.show()

------------------------------------------------------------------------

# ===== Single Cell: Monthly Sales Plot with Lebaran (April) Markers =====

import polars as pl
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------------------
# 1. PREPARE DATA
# ---------------------------
# Assumes: so_fcst_df exists
# Required columns:
# - "periods" (format: "YYYY MM")
# - "so_nw_ct" (sales value)
# - "key" (format assumed: something_soldto_shipto or similar, separated by "_")

df = so_fcst_df

# --- Extract soldto and shipto from key ---
# Adjust index positions below if needed
# Example: if key = "material_soldto_shipto"
# then soldto_index = 1, shipto_index = 2

soldto_index = 1
shipto_index = 2

df = df.with_columns([
    pl.col("key").str.split("_").list.get(soldto_index).alias("soldto"),
    pl.col("key").str.split("_").list.get(shipto_index).alias("shipto")
])

# ---------------------------
# 2. MANUAL FILTER (EDIT HERE IF NEEDED)
# ---------------------------

# OPTION A: No filter → comment both lines below
# df = df.filter(pl.col("soldto") == "YOUR_SOLDTO_CODE")
# df = df.filter(pl.col("shipto") == "YOUR_SHIPTO_CODE")

# OPTION B: Multiple values
# df = df.filter(pl.col("soldto").is_in(["A123", "B456"]))

# ---------------------------
# 3. AGGREGATE MONTHLY SALES
# ---------------------------

monthly = (
    df.group_by("periods")
      .agg(pl.col("so_nw_ct").sum().alias("monthly_sales"))
      .sort("periods")
)

monthly_pd = monthly.to_pandas()
monthly_pd["period_dt"] = pd.to_datetime(monthly_pd["periods"], format="%Y %m")

# Restrict to 2023–2025
monthly_pd = monthly_pd[
    (monthly_pd["period_dt"] >= "2023-01-01") &
    (monthly_pd["period_dt"] <= "2025-12-31")
].copy()

monthly_pd = monthly_pd.sort_values("period_dt")

# ---------------------------
# 4. PLOT
# ---------------------------

plt.figure(figsize=(14,5))
sns.lineplot(data=monthly_pd, x="period_dt", y="monthly_sales", marker="o")

plt.title("Monthly Sales (2023–2025) with Lebaran = April")
plt.xlabel("Period")
plt.ylabel("Total Sales (so_nw_ct)")

# Lebaran assumed April (month=4)
for year in [2023, 2024, 2025]:
    lebaran_date = pd.Timestamp(f"{year}-04-01")
    plt.axvline(lebaran_date, linestyle="--")
    plt.text(
        lebaran_date,
        monthly_pd["monthly_sales"].max() * 1.02,
        f"Lebaran {year}",
        ha="center",
        va="bottom",
        fontsize=9
    )

plt.tight_layout()
plt.show()


------------------------------------------------------------------------

# ===== Task 2: Compare 1 Month Before (March) vs During Lebaran (April) =====

import polars as pl
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assumes so_fcst_df exists with:
# - periods (format: "YYYY MM")
# - so_nw_ct
# - key

df = so_fcst_df

# ---------------------------
# 1. Prepare datetime
# ---------------------------
df = df.with_columns(
    pl.col("periods").str.strptime(pl.Date, format="%Y %m").alias("period_dt")
)

df = df.with_columns([
    pl.col("period_dt").dt.year().alias("year"),
    pl.col("period_dt").dt.month().alias("month")
])

# Restrict to 2023–2025
df = df.filter(pl.col("year").is_in([2023, 2024, 2025]))

# ---------------------------
# 2. OPTIONAL: Pareto Filter
# ---------------------------
# If you want Pareto filtering:
# Set PARETO_RATIO to something like 0.8 (top 80% contributors)
# If not needed, set to None

PARETO_RATIO = None   # Example: 0.8

if PARETO_RATIO is not None:
    total_by_key = (
        df.group_by("key")
          .agg(pl.col("so_nw_ct").sum().alias("total_sales"))
          .sort("total_sales", descending=True)
    )

    total_sum = total_by_key["total_sales"].sum()

    total_by_key = total_by_key.with_columns(
        (pl.col("total_sales").cumsum() / total_sum).alias("cum_ratio")
    )

    top_keys = total_by_key.filter(pl.col("cum_ratio") <= PARETO_RATIO)["key"]

    df = df.filter(pl.col("key").is_in(top_keys))

# ---------------------------
# 3. Get March (t-1) and April (Lebaran)
# ---------------------------

lebaran_month = 4
before_month = 3

comparison = (
    df.filter(pl.col("month").is_in([before_month, lebaran_month]))
      .group_by(["key", "year", "month"])
      .agg(pl.col("so_nw_ct").sum().alias("sales"))
)

comparison_pd = comparison.to_pandas()

# Pivot so we get columns: March and April
pivot = comparison_pd.pivot_table(
    index=["key", "year"],
    columns="month",
    values="sales",
    fill_value=0
).reset_index()

pivot.columns.name = None
pivot = pivot.rename(columns={
    before_month: "before_sales",
    lebaran_month: "lebaran_sales"
})

# Calculate difference
pivot["difference"] = pivot["lebaran_sales"] - pivot["before_sales"]
pivot["pct_change"] = (
    pivot["difference"] / pivot["before_sales"].replace(0, pd.NA)
)

print("Sample comparison:")
display(pivot.head())

# ---------------------------
# 4. Aggregate Overall Impact
# ---------------------------

summary = pivot.groupby("year")[["before_sales", "lebaran_sales"]].sum().reset_index()

print("Year-level summary:")
display(summary)

# ---------------------------
# 5. Plot Before vs During
# ---------------------------

plt.figure(figsize=(10,5))
sns.lineplot(data=summary, x="year", y="before_sales", marker="o", label="March (t-1)")
sns.lineplot(data=summary, x="year", y="lebaran_sales", marker="o", label="April (Lebaran)")

plt.title("Total Sales: 1 Month Before vs During Lebaran")
plt.ylabel("Total Sales (so_nw_ct)")
plt.xlabel("Year")
plt.tight_layout()
plt.show()



comparison = (
    df.filter(pl.col("month").is_in([3, 4]))
      .group_by(["year", "month"])
      .agg(pl.col("so_nw_ct").sum().alias("sales"))
)

comparison_pd = comparison.to_pandas()

pivot = comparison_pd.pivot(
    index="year",
    columns="month",
    values="sales"
).reset_index()

pivot = pivot.rename(columns={
    3: "March",
    4: "April"
})

pivot = pivot.fillna(0)

print("Year-level totals:")
display(pivot)

# -------------------------------------------------
# 6. Grouped Bar Chart
# -------------------------------------------------

x = np.arange(len(pivot["year"]))
width = 0.35

plt.figure(figsize=(10,5))

plt.bar(x - width/2, pivot["March"], width, label="March (t-1)")
plt.bar(x + width/2, pivot["April"], width, label="April (Lebaran)")

plt.xticks(x, pivot["year"])
plt.ylabel("Total Sales (so_nw_ct)")
plt.title("Sales Comparison: March vs April (Lebaran)")
plt.legend()

plt.tight_layout()
plt.show()


------------------------------------------------------------------------

import polars as pl
import pandas as pd

df = so_fcst_df

# -------------------------------
# 1. Extract components from key
# Adjust if positions differ
# -------------------------------
article_index = 0
soldto_index = 1
shipto_index = 2

df = df.with_columns([
    pl.col("key").str.split("_").list.get(article_index).alias("article"),
    pl.col("key").str.split("_").list.get(soldto_index).alias("soldto"),
    pl.col("key").str.split("_").list.get(shipto_index).alias("shipto"),
])

# -------------------------------
# 2. Manual Filters (edit as needed)
# -------------------------------

# df = df.filter(pl.col("soldto") == "XXXX")
# df = df.filter(pl.col("shipto") == "XXXX")
# df = df.filter(pl.col("article") == "XXXX")

# -------------------------------
# 3. Date Columns
# -------------------------------

df = df.with_columns(
    pl.col("periods").str.strptime(pl.Date, format="%Y %m").alias("period_dt")
)

df = df.with_columns([
    pl.col("period_dt").dt.year().alias("year"),
    pl.col("period_dt").dt.month().alias("month")
])

df = df.filter(pl.col("year").is_in([2023, 2024, 2025]))

# -------------------------------
# 4. Optional Pareto
# -------------------------------

PARETO_RATIO = None   # Example: 0.8

if PARETO_RATIO is not None:
    total_by_key = (
        df.group_by("key")
          .agg(pl.col("so_nw_ct").sum().alias("total_sales"))
          .sort("total_sales", descending=True)
    )

    total_sum = total_by_key["total_sales"].sum()

    total_by_key = total_by_key.with_columns(
        (pl.col("total_sales").cumsum() / total_sum).alias("cum_ratio")
    )

    top_keys = total_by_key.filter(pl.col("cum_ratio") <= PARETO_RATIO)["key"]

    df = df.filter(pl.col("key").is_in(top_keys))

df.head()

------------------------------------------------------------------------

# Focus months: Feb (2), Mar (3), Apr (4)

focus_months = [2, 3, 4]

comparison = (
    df.filter(pl.col("month").is_in(focus_months))
      .group_by(["year", "month"])
      .agg(pl.col("so_nw_ct").sum().alias("sales"))
)

comparison_pd = comparison.to_pandas()

pivot = comparison_pd.pivot(
    index="year",
    columns="month",
    values="sales"
).reset_index()

pivot = pivot.rename(columns={
    2: "February",
    3: "March",
    4: "April"
})

pivot = pivot.fillna(0)

pivot

------------------------------------------------------------------------

import matplotlib.pyplot as plt
import numpy as np

x = np.arange(len(pivot["year"]))
width = 0.25

plt.figure(figsize=(11,5))

plt.bar(x - width, pivot["February"], width, label="February")
plt.bar(x, pivot["March"], width, label="March")
plt.bar(x + width, pivot["April"], width, label="April (Lebaran)")

plt.xticks(x, pivot["year"])
plt.ylabel("Total Sales (so_nw_ct)")
plt.title("Sales Comparison: February vs March vs April (Lebaran)")
plt.legend()

plt.tight_layout()
plt.show()

------------------------------------------------------------------------

import polars as pl
import pandas as pd

df = so_fcst_df

# ----------------------------
# Date preparation
# ----------------------------
df = df.with_columns(
    pl.col("periods").str.strptime(pl.Date, format="%Y %m").alias("period_dt")
)

df = df.with_columns([
    pl.col("period_dt").dt.year().alias("year"),
    pl.col("period_dt").dt.month().alias("month")
])

df = df.filter(pl.col("year").is_in([2023, 2024, 2025]))
df = df.filter(pl.col("month").is_in([1,2,3,4]))

# ----------------------------
# Aggregate national totals
# ----------------------------

monthly = (
    df.group_by(["year", "month"])
      .agg(pl.col("so_nw_ct").sum().alias("sales"))
)

monthly_pd = monthly.to_pandas()

pivot = monthly_pd.pivot(
    index="year",
    columns="month",
    values="sales"
).reset_index()

pivot = pivot.rename(columns={
    1: "Jan",
    2: "Feb",
    3: "Mar",
    4: "Apr"
}).fillna(0)

pivot



# ----------------------------
# Year-level differences
# ----------------------------

pivot["3M_before_minus_during"] = pivot["Jan"] - pivot["Apr"]
pivot["1M_before_minus_during"] = pivot["Mar"] - pivot["Apr"]
pivot["2M_before_minus_1M_before"] = pivot["Feb"] - pivot["Mar"]
pivot["3M_before_minus_2M_before"] = pivot["Jan"] - pivot["Feb"]

print("Year-level National Differences:")
display(pivot)

# ----------------------------
# Overall National Number (2023–2025 combined)
# ----------------------------

overall = pivot[["Jan","Feb","Mar","Apr"]].sum()

overall_diff = pd.DataFrame({
    "Metric": [
        "3M_before_minus_during",
        "1M_before_minus_during",
        "2M_before_minus_1M_before",
        "3M_before_minus_2M_before"
    ],
    "Value": [
        overall["Jan"] - overall["Apr"],
        overall["Mar"] - overall["Apr"],
        overall["Feb"] - overall["Mar"],
        overall["Jan"] - overall["Feb"]
    ]
})

print("Combined 2023–2025 National Numbers:")
display(overall_diff)

------------------------------------------------------------------------

# ----------------------------
# Percentage Differences (ratio form)
# ----------------------------

pivot["3M_before_minus_during_pct"] = (
    (pivot["Jan"] - pivot["Apr"]) / pivot["Apr"].replace(0, pd.NA)
)

pivot["1M_before_minus_during_pct"] = (
    (pivot["Mar"] - pivot["Apr"]) / pivot["Apr"].replace(0, pd.NA)
)

pivot["2M_before_minus_1M_before_pct"] = (
    (pivot["Feb"] - pivot["Mar"]) / pivot["Mar"].replace(0, pd.NA)
)

pivot["3M_before_minus_2M_before_pct"] = (
    (pivot["Jan"] - pivot["Feb"]) / pivot["Feb"].replace(0, pd.NA)
)

pivot

------------------------------------------------------------------------

overall_pct = pd.DataFrame({
    "Metric": [
        "3M_before_minus_during_pct",
        "1M_before_minus_during_pct",
        "2M_before_minus_1M_before_pct",
        "3M_before_minus_2M_before_pct"
    ],
    "Value": [
        (overall["Jan"] - overall["Apr"]) / overall["Apr"] if overall["Apr"] != 0 else None,
        (overall["Mar"] - overall["Apr"]) / overall["Apr"] if overall["Apr"] != 0 else None,
        (overall["Feb"] - overall["Mar"]) / overall["Mar"] if overall["Mar"] != 0 else None,
        (overall["Jan"] - overall["Feb"]) / overall["Feb"] if overall["Feb"] != 0 else None
    ]
})

print("Combined 2023–2025 National Percentage (ratio form):")
display(overall_pct)

------------------------------------------------------------------------

zone_monthly = (
    df_base
    .group_by(["zone", "year", "month"])
    .agg(pl.col("so_nw_ct").sum().alias("sales"))
)

zone_pd = zone_monthly.to_pandas()

pivot_zone = zone_pd.pivot_table(
    index=["zone", "year"],
    columns="month",
    values="sales",
    fill_value=0
).reset_index()

pivot_zone = pivot_zone.rename(columns={
    1: "Jan",
    2: "Feb",
    3: "Mar",
    4: "Apr"
})

# Compute differences (same logic as Number 4)

pivot_zone["3M - Lebaran"] = pivot_zone["Jan"] - pivot_zone["Apr"]
pivot_zone["1M - Lebaran"] = pivot_zone["Mar"] - pivot_zone["Apr"]
pivot_zone["2M - 1M"] = pivot_zone["Feb"] - pivot_zone["Mar"]
pivot_zone["3M - 2M"] = pivot_zone["Jan"] - pivot_zone["Feb"]

pivot_zone.head()

--------------------------------------------------------------------------

zone_avg = (
    pivot_zone
    .groupby("zone")[[
        "3M - Lebaran",
        "1M - Lebaran",
        "2M - 1M",
        "3M -2M"
    ]]
    .mean()
    .reset_index()
)

zone_avg = zone_avg.sort_values("1M - Lebaran    ")

zone_avg





pivot_zone["3M_before_minus_during_ratio"] = (
    (pivot_zone["Jan"] - pivot_zone["Apr"]) /
    pivot_zone["Apr"].replace(0, pd.NA)
)

pivot_zone["1M_before_minus_during_ratio"] = (
    (pivot_zone["Mar"] - pivot_zone["Apr"]) /
    pivot_zone["Apr"].replace(0, pd.NA)
)

pivot_zone["2M_before_minus_1M_before_ratio"] = (
    (pivot_zone["Feb"] - pivot_zone["Mar"]) /
    pivot_zone["Mar"].replace(0, pd.NA)
)

pivot_zone["3M_before_minus_2M_before_ratio"] = (
    (pivot_zone["Jan"] - pivot_zone["Feb"]) /
    pivot_zone["Feb"].replace(0, pd.NA)
)


pivot_zone.to_csv("pivot_zone_full.csv", index=False)

print("pivot_zone_full.csv saved.")





# ---------------------------------
# Reusable Pareto Filter Function
# ---------------------------------

def apply_pareto(df_input, ratio):
    """
    Returns filtered dataframe containing top cumulative ratio contributors.
    ratio = 0.8 keeps top 80% cumulative sales.
    """

    total_by_key = (
        df_input
        .group_by("key")
        .agg(pl.col("so_nw_ct").sum().alias("total_sales"))
        .sort("total_sales", descending=True)
    )

    total_sum = total_by_key["total_sales"].sum()

    total_by_key = total_by_key.with_columns(
        (pl.col("total_sales").cumsum() / total_sum).alias("cum_ratio")
    )

    top_keys = total_by_key.filter(
        pl.col("cum_ratio") <= ratio
    )["key"]

    return df_input.filter(pl.col("key").is_in(top_keys))


# Toggle this when needed
PARETO_RATIO = None   # Example: 0.8

if PARETO_RATIO is not None:
    df_working = apply_pareto(df_base, PARETO_RATIO)
else:
    df_working = df_base

print("Using Pareto:" , PARETO_RATIO)

---------------------------------------------------------------------------------------------------
# Cell A — per-key Jan–Apr pivot, differences, ratios; save CSVs
import polars as pl
import pandas as pd
import numpy as np

# --- Config (edit only this if your key structure differs) ---
zone_index = 0          # index of zone inside key when splitting by '_' (default 0)
years_keep = [2023, 2024, 2025]
months_keep = [1, 2, 3, 4]   # Jan, Feb, Mar, Apr

# --- Source (uses df_pareto as you requested) ---
df = df_pareto  # must exist

# --- Ensure period, year, month exist ---
if "period_dt" not in df.columns:
    df = df.with_columns(pl.col("periods").str.strptime(pl.Date, format="%Y %m").alias("period_dt"))
if "year" not in df.columns or "month" not in df.columns:
    df = df.with_columns([
        pl.col("period_dt").dt.year().alias("year"),
        pl.col("period_dt").dt.month().alias("month")
    ])

# Filter to target years and months
df = df.filter(pl.col("year").is_in(years_keep) & pl.col("month").is_in(months_keep))

# Aggregate sales per key-year-month
key_monthly = (
    df
    .group_by(["key", "year", "month"])
    .agg(pl.col("so_nw_ct").sum().alias("sales"))
)

# Convert to pandas pivot with columns for months 1..4
key_monthly_pd = key_monthly.to_pandas()
pivot_key = key_monthly_pd.pivot_table(
    index=["key", "year"],
    columns="month",
    values="sales",
    fill_value=0
).reset_index()

# Make sure month columns exist always (1..4)
for m in months_keep:
    if m not in pivot_key.columns:
        pivot_key[m] = 0

pivot_key = pivot_key.rename(columns={1: "Jan", 2: "Feb", 3: "Mar", 4: "Apr"})
pivot_key = pivot_key[["key", "year", "Jan", "Feb", "Mar", "Apr"]]

# Add zone column by splitting key (pandas)
pivot_key["zone"] = pivot_key["key"].str.split("_").str.get(zone_index)

# Compute absolute differences (per your spec)
pivot_key["3M_before_minus_during"] = pivot_key["Jan"] - pivot_key["Apr"]
pivot_key["1M_before_minus_during"] = pivot_key["Mar"] - pivot_key["Apr"]
pivot_key["2M_before_minus_1M_before"] = pivot_key["Feb"] - pivot_key["Mar"]
pivot_key["3M_before_minus_2M_before"] = pivot_key["Jan"] - pivot_key["Feb"]

# Compute ratio (decimal) differences; denominator replaced with NA if zero to avoid inf
pivot_key["3M_before_minus_during_ratio"] = (pivot_key["Jan"] - pivot_key["Apr"]) / pivot_key["Apr"].replace(0, np.nan)
pivot_key["1M_before_minus_during_ratio"] = (pivot_key["Mar"] - pivot_key["Apr"]) / pivot_key["Apr"].replace(0, np.nan)
pivot_key["2M_before_minus_1M_before_ratio"] = (pivot_key["Feb"] - pivot_key["Mar"]) / pivot_key["Mar"].replace(0, np.nan)
pivot_key["3M_before_minus_2M_before_ratio"] = (pivot_key["Jan"] - pivot_key["Feb"]) / pivot_key["Feb"].replace(0, np.nan)

# Save full per-key pivot (so you can inspect all key×year rows)
pivot_key.to_csv("pivot_key_jan_apr_full.csv", index=False)
print("Saved pivot_key_jan_apr_full.csv (per-key × year details).")

# Also build pivot_zone (zone × year × months) and save (summary)
pivot_zone = pivot_key.groupby(["zone", "year"], as_index=False).agg({
    "Jan": "sum",
    "Feb": "sum",
    "Mar": "sum",
    "Apr": "sum",
    "3M_before_minus_during": "sum",
    "1M_before_minus_during": "sum",
    "2M_before_minus_1M_before": "sum",
    "3M_before_minus_2M_before": "sum"
})

# Recompute ratios at zone-year level safely (denominator zero -> NaN)
pivot_zone["3M_before_minus_during_ratio"] = (pivot_zone["Jan"] - pivot_zone["Apr"]) / pivot_zone["Apr"].replace(0, np.nan)
pivot_zone["1M_before_minus_during_ratio"] = (pivot_zone["Mar"] - pivot_zone["Apr"]) / pivot_zone["Apr"].replace(0, np.nan)
pivot_zone["2M_before_minus_1M_before_ratio"] = (pivot_zone["Feb"] - pivot_zone["Mar"]) / pivot_zone["Mar"].replace(0, np.nan)
pivot_zone["3M_before_minus_2M_before_ratio"] = (pivot_zone["Jan"] - pivot_zone["Feb"]) / pivot_zone["Feb"].replace(0, np.nan)

pivot_zone.to_csv("pivot_zone_jan_apr_full.csv", index=False)
print("Saved pivot_zone_jan_apr_full.csv (zone × year details).")

# Keep pivot_key and pivot_zone in memory for next cell
print("Rows (key×year):", len(pivot_key))
print("Rows (zone×year):", len(pivot_zone))


--------------------------------------------------------------------------------------------------------------------


# Cell B — national & zone averages + plots
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# pivot_key and pivot_zone are loaded from previous cell (pandas DataFrames)

# --- NATIONAL AVERAGES (mean across keys) PER YEAR ---
national_by_year = pivot_key.groupby("year").agg({
    "3M_before_minus_during": "mean",
    "1M_before_minus_during": "mean",
    "2M_before_minus_1M_before": "mean",
    "3M_before_minus_2M_before": "mean",
    "3M_before_minus_during_ratio": "mean",
    "1M_before_minus_during_ratio": "mean",
    "2M_before_minus_1M_before_ratio": "mean",
    "3M_before_minus_2M_before_ratio": "mean"
}).reset_index()

print("National averages per year (unweighted mean across keys):")
display(national_by_year)

# --- NATIONAL COMBINED (2023-2025) AVERAGE ACROSS ALL key×year ROWS ---
national_combined = pivot_key[[
    "3M_before_minus_during",
    "1M_before_minus_during",
    "2M_before_minus_1M_before",
    "3M_before_minus_2M_before",
    "3M_before_minus_during_ratio",
    "1M_before_minus_during_ratio",
    "2M_before_minus_1M_before_ratio",
    "3M_before_minus_2M_before_ratio"
]].mean(skipna=True).rename("combined_mean").to_frame().reset_index()
national_combined.columns = ["metric", "value"]

print("National combined mean (2023–2025) across all key×year rows:")
display(national_combined)

# --- ZONE AVERAGES (per year and combined) ---
zone_by_year = pivot_key.groupby(["zone", "year"]).agg({
    "3M_before_minus_during": "mean",
    "1M_before_minus_during": "mean",
    "2M_before_minus_1M_before": "mean",
    "3M_before_minus_2M_before": "mean",
    "3M_before_minus_during_ratio": "mean",
    "1M_before_minus_during_ratio": "mean",
    "2M_before_minus_1M_before_ratio": "mean",
    "3M_before_minus_2M_before_ratio": "mean"
}).reset_index()

print("Zone averages per year (mean across keys in zone):")
display(zone_by_year)

zone_combined = zone_by_year.groupby("zone").mean(numeric_only=True).reset_index()
print("Zone averages combined across years:")
display(zone_combined)

# --- Quick Plots (national_by_year) ---
# Plot absolute differences per year (grouped bars)
plt.figure(figsize=(10,5))
x = np.arange(len(national_by_year["year"]))
w = 0.2
plt.bar(x - 1.5*w, national_by_year["3M_before_minus_during"], width=w, label="Jan - Apr")
plt.bar(x - 0.5*w, national_by_year["1M_before_minus_during"], width=w, label="Mar - Apr")
plt.bar(x + 0.5*w, national_by_year["2M_before_minus_1M_before"], width=w, label="Feb - Mar")
plt.bar(x + 1.5*w, national_by_year["3M_before_minus_2M_before"], width=w, label="Jan - Feb")
plt.xticks(x, national_by_year["year"])
plt.ylabel("Average absolute difference (per key)")
plt.title("National avg differences per year (mean across keys)")
plt.legend()
plt.tight_layout()
plt.show()

# Plot ratio metrics per year (grouped)
plt.figure(figsize=(10,5))
plt.bar(x - 1.5*w, national_by_year["3M_before_minus_during_ratio"], width=w, label="(Jan-Apr)/Apr")
plt.bar(x - 0.5*w, national_by_year["1M_before_minus_during_ratio"], width=w, label="(Mar-Apr)/Apr")
plt.bar(x + 0.5*w, national_by_year["2M_before_minus_1M_before_ratio"], width=w, label="(Feb-Mar)/Mar")
plt.bar(x + 1.5*w, national_by_year["3M_before_minus_2M_before_ratio"], width=w, label="(Jan-Feb)/Feb")
plt.xticks(x, national_by_year["year"])
plt.ylabel("Average ratio (decimal, mean across keys)")
plt.title("National avg ratio metrics per year")
plt.legend()
plt.tight_layout()
plt.show()

# --- Save summaries for reporting ---
national_by_year.to_csv("national_by_year_jan_apr_avg.csv", index=False)
national_combined.to_csv("national_combined_jan_apr_avg.csv", index=False)
zone_by_year.to_csv("zone_by_year_jan_apr_avg.csv", index=False)
zone_combined.to_csv("zone_combined_jan_apr_avg.csv", index=False)

print("Saved summary CSVs: national_by_year_jan_apr_avg.csv, national_combined_jan_apr_avg.csv, zone_by_year_jan_apr_avg.csv, zone_combined_jan_apr_avg.csv")
---------------------------------------------------------------------------------------------------------------------------

if "period_dt" not in df_pareto.columns:
    df_pareto = df_pareto.with_columns(
        pl.col("periods").str.strptime(pl.Date, format="%Y %m").alias("period_dt")
    )

# monthly aggregation (sum of so_nw_ct per key x period_dt)
monthly_df = (
    df_pareto
    .groupby(["key", "period_dt", "zone"], maintain_order=False)
    .agg(pl.col("so_nw_ct").sum().alias("so_nw_ct"))
    .sort(["key", "period_dt"])
)
# sanity
print("monthly_df rows:", monthly_df.shape[0])
monthly_df.head()

---------------------------------------------------------------------------------------------------------------------------
# Cell 2: build (key x lebaran period x offset) rows and join monthly sales
# Offsets we need: 0 (lebaran month), -1, -2, -3
offsets = [0, -1, -2, -3]

# get unique keys and zones from df_pareto
keys_zones = df_pareto.select(["key", "zone"]).unique().to_dicts()

# build target rows in python (safe and straightforward)
rows = []
for rec in keys_zones:
    key = rec["key"]
    zone = rec["zone"]
    for year, month in lebaran_month_cfg.items():
        lebaran_dt = datetime(year, month, 1)
        for off in offsets:
            rows.append({
                "key": key,
                "zone": zone,
                "lebaran_year": year,
                "lebaran_month": month,
                "offset": off,
                "target_period_dt": (lebaran_dt + relativedelta(months=off)).date()
            })

target_df = pl.DataFrame(rows)

# convert monthly_df.period_dt to python.date to ensure join compatibility (if needed)
# monthly_df.period_dt may be polars date type; convert to python date for join key consistency
monthly_join = monthly_df.with_columns(
    pl.col("period_dt").apply(lambda d: d.to_python().date() if d is not None else None).alias("period_date")
)

# join on key + period_date == target_period_dt
target_with_sales = (
    target_df
    .join(
        monthly_join.rename({"period_date": "target_period_dt"}),
        on=["key", "zone", "target_period_dt"],
        how="left"
    )
    .select([
        "key", "zone", "lebaran_year", "lebaran_month", "offset", 
        pl.col("target_period_dt").alias("period_dt"),
        pl.col("so_nw_ct")
    ])
    .with_columns(pl.col("so_nw_ct").fill_null(0))
)

# pivot offsets into columns m_0 (lebaran), m_1 (1 month before), etc.
# we will map offsets->names: 0 -> 'lebaran', -1 -> 'm_1', -2 -> 'm_2', -3 -> 'm_3'
offset_name_map = {0: "lebaran", -1: "m_1", -2: "m_2", -3: "m_3"}
target_pivot = (
    target_with_sales
    .with_columns(pl.col("offset").apply(lambda x: offset_name_map[x]).alias("offset_name"))
    .pivot(values="so_nw_ct", index=["key", "zone", "lebaran_year", "lebaran_month"], columns="offset_name", aggregate_function="sum")
    .with_columns([
        # ensure missing month columns exist (fill with 0 if missing)
        pl.when(pl.col("lebaran").is_null()).then(0).otherwise(pl.col("lebaran")).alias("lebaran"),
        pl.when(pl.col("m_1").is_null()).then(0).otherwise(pl.col("m_1")).alias("m_1"),
        pl.when(pl.col("m_2").is_null()).then(0).otherwise(pl.col("m_2")).alias("m_2"),
        pl.when(pl.col("m_3").is_null()).then(0).otherwise(pl.col("m_3")).alias("m_3"),
    ])
    .sort(["lebaran_year", "key"])
)

print("target_pivot rows:", target_pivot.shape[0])
target_pivot.head()

---------------------------------------------------------------------------------------------------------------------------
# Cell 3: compute diffs and percentage ratios (handle zero denominators safely)
tp = target_pivot

# differences (absolute)
tp = tp.with_columns([
    (pl.col("m_3") - pl.col("lebaran")).alias("d_3m_minus_lebaran"),
    (pl.col("m_1") - pl.col("lebaran")).alias("d_1m_minus_lebaran"),
    (pl.col("m_2") - pl.col("m_1")).alias("d_2m_minus_1m"),
    (pl.col("m_3") - pl.col("m_2")).alias("d_3m_minus_2m"),
])

# percentage ratios: choose denominator per difference:
# - for ..._minus_lebaran -> denom = lebaran
# - for 2m_minus_1m -> denom = m_1
# - for 3m_minus_2m -> denom = m_2
tp = tp.with_columns([
    pl.when(pl.col("lebaran") > 0)
      .then((pl.col("d_3m_minus_lebaran") / pl.col("lebaran")) * 100)
      .otherwise(None)
      .alias("pct_3m_vs_lebaran(%)"),

    pl.when(pl.col("lebaran") > 0)
      .then((pl.col("d_1m_minus_lebaran") / pl.col("lebaran")) * 100)
      .otherwise(None)
      .alias("pct_1m_vs_lebaran(%)"),

    pl.when(pl.col("m_1") > 0)
      .then((pl.col("d_2m_minus_1m") / pl.col("m_1")) * 100)
      .otherwise(None)
      .alias("pct_2m_vs_1m(%)"),

    pl.when(pl.col("m_2") > 0)
      .then((pl.col("d_3m_minus_2m") / pl.col("m_2")) * 100)
      .otherwise(None)
      .alias("pct_3m_vs_2m(%)"),
])

# average the available percentage ratios per (key,lebaran_year)
# average should ignore nulls (we'll compute mean across the four pct columns)
pct_cols = ["pct_3m_vs_lebaran(%)", "pct_1m_vs_lebaran(%)", "pct_2m_vs_1m(%)", "pct_3m_vs_2m(%)"]

# create a horizontal mean that ignores nulls
tp = tp.with_columns(
    pl.concat_list([pl.col(c) for c in pct_cols]).alias("pct_list")
).with_columns(
    pl.col("pct_list").arr.eval(pl.element(), parallel=True).alias("pct_list_eval")  # ensure array
).with_columns(
    pl.when(pl.col("pct_list").arr.lengths() > 0)
      .then(pl.col("pct_list").arr.mean().round(2))
      .otherwise(None)
      .alias("avg_pct_all_four(%)")
).drop(["pct_list", "pct_list_eval"])

# final per-key, per-lebaran-year table
per_key_lebaran = tp.select([
    "key", "zone", "lebaran_year", "lebaran_month", "lebaran", "m_1", "m_2", "m_3",
    *["d_3m_minus_lebaran", "d_1m_minus_lebaran", "d_2m_minus_1m", "d_3m_minus_2m"],
    *pct_cols,
    "avg_pct_all_four(%)"
])

per_key_lebaran.head(10)

---------------------------------------------------------------------------------------------------------------------------


---------------------------------------------------------------------------------------------------------------------------


---------------------------------------------------------------------------------------------------------------------------
